A inteligência artificial (IA), surgida na década de 1950, tem sua origem praticamente confundida com a própria origem do computador. Mais precisamente, no verão de 1956, ocorreu a Darthmouth College Conference,1 que é considerada o marco inicial da IA. Os pesquisadores reconhecidos como pais da área, como John MacCarthy, Marvin Minsky, Alan Newell e Herbert Simon, entre outros, participaram desse evento e tiveram trajetórias científicas que estabeleceram marcos nesse fascinante domínio da Computação.

Como o nome mesmo insinua, a área sempre foi cercada de enormes expectativas, e em inúmeras vezes essas não foram completamente atingidas. Desse modo, a oscilação de humor em relação à área assemelha-se a uma curva senoidal, havendo períodos de grande entusiasmo e grande financiamento (como ocorre agora) seguidos por outros de decepção e recursos escassos. Estes últimos são conhecidos como AI Winter (Inverno da IA), como foram por exemplo os períodos entre 1975/1980 e 1987/1993.

Atualmente, atravessamos novamente um período de euforia sobre os possíveis benefícios que a IA pode prover. Tal otimismo se justifica por uma conjunção de três fatores fundamentais: (i) o custo de processamento e de memória nunca foi tão barato; (ii) o surgimento de novos paradigmas, como as redes neurais profundas, possibilitados pelo primeiro fator e produzindo inegáveis avanços científicos; e (iii) uma quantidade de dados gigantesca disponível na internet em razão do grande uso de recursos tais como redes e mídias sociais. Tal entusiasmo, entretanto, vem sido acompanhado por uma série de temores, alguns dos quais fundados.

O objetivo deste artigo é prover informações para que o leitor comum possa melhor entender os principais aspectos da IA, em que ela difere da computação convencional e como ela pode ser inserida nos processos organizacionais da sociedade humana. Além disso, busca evidenciar os grandes avanços e potenciais riscos que essa tecnologia, tal como qualquer outra, pode provocar caso os atores envolvidos na produção, utilização e regulação de seu uso não criem um espaço de discussão adequado destas questões.

Sempre que ocorre um entusiasmo com os resultados de uma tecnologia, existe uma tendência da mídia em fornecer definições e explicações, por vezes não muito precisas, dos seus principais aspectos. Isso é, certamente, o que ocorre com a IA nos dias de hoje.

Em primeiro lugar, cabe ressaltar que não existe uma definição acadêmica, propriamente dita, do que vem a ser IA. Trata-se certamente de um ramo da ciência/engenharia da computação, e portanto visa desenvolver sistemas computacionais que solucionam problemas. Para tal, utiliza um número diverso de técnicas e modelos, dependendo dos problemas abordados. Portanto, é inadequado utilizar-se expressões como “a IA da empresa X”; mais adequado (porém com menos apelo) seria dizer “um sistema da empresa X que utiliza técnicas de IA”.

Ao invés de tentar fornecer uma definição de IA, mais adequado seria tentar caracterizar quais são os objetivos da área. Uma das primeiras tentativas desta abordagem, proposta em Rich e Knight (1991), é a seguinte: o objetivo da IA é desenvolver sistemas para realizar tarefas que, no momento: (i) são mais bem realizadas por seres humanos que por máquinas, ou (ii) não possuem solução algorítmica viável pela computação convencional.

Para entender melhor essa definição, necessita-se esclarecer o que vem a ser um algoritmo, palavra que também é bastante citada na mídia, às vezes de modo não muito preciso. Um algoritmo nada mais é do que uma sequência finita de ações que resolve um certo problema. Uma receita culinária, como a de um risoto, é um algoritmo. Assim, um algoritmo pode resolver problemas de tipos bastante diferentes: cálculo estrutural (projeto de uma ponte), processamento de dados (geração de uma folha de pagamentos) ou planejamento (definição de um pacote de turismo).

Qual a principal diferença entre esses problemas? Basicamente, certos problemas têm soluções exatas, como o projeto da ponte, o processamento da folha de pagamentos e a receita do risoto. Solução exata, nesse caso, significa que se os passos definidos no algoritmo forem executados exatamente na ordem definida, ter-se-á ao final uma ponte que resistirá às intempéries, uma folha de pagamentos sem futuros problemas com o fisco e um delicioso risoto à moda italiana.

Por outro lado, problemas como a definição do pacote de turismo não têm uma solução exata, ou uma única solução. Outros exemplos similares são produção de diagnósticos (médicos, legais), geração automática de diálogos, reconhecimento de imagens etc. No caso do pacote de turismo, como garantir que é o melhor a ser adquirido? Deve-se escolher primeiro o voo ou o hotel? Quais datas teriam um custo menor? Existe disponibilidade nessas datas para todos os recursos desejados (hotéis, voos, passeios), e em caso positivo as férias podem ser marcadas nesse período?

Uma possível abordagem para solucionar tais problemas seria tentar gerar as possíveis soluções até que se obtenha a primeira delas, ou até que se encontre a melhor delas, caso existam várias soluções. Tal abordagem, apesar de teoricamente plausível, quase sempre é inviável na prática: a quantidade de possíveis soluções geradas é muito grande, e mesmo com um computador muito potente levaria muito tempo para obtê-las. Por exemplo, um problema de definição de rotas entre cidades poderia levar centenas de dias de processamento!2

Assim, tais problemas são usualmente mais bem solucionados por seres humanos, e na maioria dos casos de interesse não possuem solução algorítmica viável (em tempo de processamento) pela computação convencional.

Uma pergunta que se coloca então é a seguinte: Como nós, humanos, solucionamos esses problemas? Uma possível resposta é que utilizamos, de modo inato, um mecanismo de busca e poda: (i) geramos soluções candidatas ... mas quase nunca todas elas! (ii) escolhemos a melhor solução... de acordo com certo critério! e (iii) eventualmente, analisamos a posteriori o efeito das escolhas feitas... e as alteramos para o futuro i.e., aprendemos!

Assim, o domínio de IA se caracteriza por ser uma coleção de modelos, técnicas e tecnologias (busca, raciocínio e representação de conhecimento, mecanismos de decisão, percepção, planejamento, processamento de linguagem natural, tratamento de incertezas, aprendizado de máquina) que, isoladamente ou agrupadas, resolvem problemas de tal natureza. Para tal, podem utilizar paradigmas distintos, sendo os principais os paradigmas simbólico, conexionista, evolutivo e probabilístico.

Segundo o paradigma simbólico, deve-se inicialmente identificar o conhecimento do domínio (modelo do problema), para então representá-lo utilizando uma linguagem formal de representação e implementar um mecanismo de inferência para utilizar esse conhecimento.

Já no paradigma conexionista, a linguagem é uma rede de elementos simples, inspirada no funcionamento do cérebro, onde neurônios artificiais, conectados em rede, são capazes de aprender e de generalizar a partir de exemplos. O raciocínio consiste em aprender diretamente a função entrada-saída. Matematicamente, trata-se de uma técnica de aproximação de funções por regressão não linear.

O paradigma evolutivo, por sua vez, utiliza um método probabilístico de busca de soluções de problemas (otimização), onde soluções são representadas como indivíduos, aos quais se aplicam técnicas “inspiradas” na teoria da evolução como hereditariedade, mutação, seleção natural e recombinação (ou crossing over), para selecionar para as gerações seguintes os indivíduos mais adaptados, i.e., os que maximizam uma função objetivo (ou fitness function).

Finalmente, o paradigma probabilístico utiliza modelos para representar o conceito estatístico de independência condicional, a partir de relacionamentos causais no domínio. A inferência consiste em calcular a distribuição condicional de probabilidades dessa distribuição, e em alguns casos particulares de topologia, existem algoritmos bastante eficientes.

Agentes inteligentes
Uma contribuição muito importante foi o surgimento do conceito de agente inteligente (Russell; Norvig, 2010), proposto em 1995, que se tornou um paradigma integrador da área. Esse paradigma gerou uma nova área de pesquisa, denominada agentes autônomos e sistemas multiagentes, dedicada a investigar como as acima mencionadas técnicas de IA poderiam ser integradas de modo mais eficaz e efetivo em um único agente e também como um conjunto destes agentes poderia interagir de forma coordenada e cooperativa, visando resolver um problema quando nenhum deles de forma isolada poderia fazê-lo. Um conjunto de veículos autônomos seria um exemplo de um sistema multiagentes: não basta que cada um decida o melhor roteiro para atingir a meta de seu passageiro, mas é necessário que os veículos cooperem e se coordenem, para não causarem acidentes, como usualmente ocorre com condutores humanos.

Nessa nova e fascinante área de pesquisa, surgiram algumas definições importantes do que seria um agente, como a inicialmente proposta por Wooldridge (1997 apud Jennings, 1999, p.1): “Um agente é um sistema computacional encapsulado que está situado em algum ambiente, e que é capaz de ação autônoma e flexível naquele ambiente, a fim de cumprir seus objetivos”.

A inserção da dimensão organizacional e a interação com os usuários foi proposta na sequência em Boissier e Sichman (2004, p.5): “Um agente é entidade real ou virtual, que é autônoma, pró-ativa, reativa e social, sendo capaz de exibir atividade organizada de modo a atingir seus objetivos, eventualmente interagindo com usuários”.

Em ambas as definições, menciona-se o conceito de autonomia, crucial para que se possa refletir sobre os possíveis efeitos positivos e negativos da interação desses sistemas com os seres humanos.